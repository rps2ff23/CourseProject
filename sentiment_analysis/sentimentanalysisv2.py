# -*- coding: utf-8 -*-
"""SentimentAnalysisv2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tlXSW76Aw8TrD6gzBkjXCzRBNy5qEfA2
"""

from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten, LSTM
from keras.layers import GlobalMaxPooling1D
from keras.models import Model
from keras.layers.embeddings import Embedding
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.layers import Input
from keras.layers.merge import Concatenate

import pandas as pd
import numpy as np
import re

import matplotlib.pyplot as plt

from numpy import array
from numpy import asarray
from numpy import zeros

from keras.models import Sequential, save_model, load_model
import matplotlib.pyplot as plt

def read_file(input_file):
  sentiment_data = pd.read_csv(input_file, error_bad_lines=False)
  sentiment_data = sentiment_data.dropna()
  
  

  tokenizer = Tokenizer(num_words = 5000)
  tokenizer.fit_on_texts(sentiment_data['comment'])
  print('Found %d unique words.' % len(tokenizer.word_index))

  #View distribution of ratings in data
  ax = sentiment_data['rating'].value_counts(sort=False).plot(kind='barh')
  ax.set_xlabel('Number of Samples in Training Set')
  ax.set_ylabel('Label')
  #plt.show()
  return sentiment_data

def preprocess_comments(comment):
    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', comment)

    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    return sentence

def create_dataset(sentiment_data):
  X = []
  sentences = list(sentiment_data["comment"])
  for sen in sentences:
      X.append(preprocess_comments(sen))

  y = sentiment_data[['strongly positive','positive','negative','strongly negative']]

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
  X_train, X_test, tokenizer = tokenize(X_train, X_test)

  y1_train = y_train[["strongly positive"]].values
  y1_test =  y_test[["strongly positive"]].values

  y2_train = y_train[["positive"]].values
  y2_test =  y_test[["positive"]].values

  y3_train = y_train[["negative"]].values
  y3_test =  y_test[["negative"]].values

  y4_train = y_train[["strongly negative"]].values
  y4_test =  y_test[["strongly negative"]].values

  return tokenizer, X_train, X_test, y1_train, y1_test, y2_train, y2_test, y3_train, y3_test, y4_train, y4_test

def tokenize(X_train, X_test):
  tokenizer = Tokenizer(num_words=3437)
  tokenizer.fit_on_texts(X_train)

  X_train = tokenizer.texts_to_sequences(X_train)
  X_test = tokenizer.texts_to_sequences(X_test)

  vocab_size = len(tokenizer.word_index) + 1

  maxlen = 50

  X_train = pad_sequences(X_train, padding='pre', maxlen=maxlen)
  X_test = pad_sequences(X_test, padding='pre', maxlen=maxlen)

  return X_train, X_test, tokenizer

def embeddings(tokenizer):
  embeddings_dictionary = dict()

  glove_file = open('glove.6B.50d.txt', encoding="utf8")

  for line in glove_file:
      records = line.split()
      word = records[0]
      vector_dimensions = asarray(records[1:], dtype='float32')
      embeddings_dictionary[word] = vector_dimensions
  glove_file.close()

  vocab_size = len(tokenizer.word_index) + 1

  embedding_matrix = zeros((vocab_size, 50))
  for word, index in tokenizer.word_index.items():
      embedding_vector = embeddings_dictionary.get(word)
      if embedding_vector is not None:
          embedding_matrix[index] = embedding_vector

  return embedding_matrix

def create_model(tokenizer, embedding_matrix):
  input_1 = Input(shape=(50,))
  vocab_size = len(tokenizer.word_index) + 1
  embedding_layer = Embedding(vocab_size, 50, weights=[embedding_matrix], trainable=False)(input_1)
  LSTM_Layer1 = LSTM(128)(embedding_layer)

  output1 = Dense(1, activation='sigmoid')(LSTM_Layer1)
  output2 = Dense(1, activation='sigmoid')(LSTM_Layer1)
  output3 = Dense(1, activation='sigmoid')(LSTM_Layer1)
  output4 = Dense(1, activation='sigmoid')(LSTM_Layer1)

  model = Model(inputs=input_1, outputs=[output1, output2, output3, output4])
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])

  return model

from keras.utils.vis_utils import plot_model

def run_model(model, X_train, y1_train, y2_train, y3_train, y4_train):
  plot_model(model, to_file='model_plot4b.png', show_shapes=True, show_layer_names=True)
  history = model.fit(X_train, y=[y1_train, y2_train, y3_train, y4_train], batch_size=128, epochs=20, verbose=1, validation_split=0.3)

def evaluate_model(model, X_test, y1_test, y2_test, y3_test, y4_test):
  score = model.evaluate(x=X_test, y=[y1_test, y2_test, y3_test, y4_test], verbose=1)

  print("Test Score:", score[0])
  print("Test Accuracy:", score[1])

def save_sentiment_model(model):
  # Save the model
  filepath = './saved_model'
  save_model(model, filepath)

def predict(model, tokenizer, input):
  # Load the model
  #model = load_model(filepath, compile = True)
  labels = ['strongly positive','positive','negative','strongly negative']
  seq = tokenizer.texts_to_sequences(input)
  #print(input)
  padded = pad_sequences(seq, maxlen=50, padding='pre')
  #print(padded)
  pred = model.predict(padded)
  #print(pred)
  print(labels[np.argmax(pred)])
  return labels[np.argmax(pred)]

def main(input_file, input, model_file):

  sample_args = input
  sample_data = ''
  for i in sample_args:
      sample_data += i + ' '
  sample_data = sample_data[0:len(sample_data)-1]

  sentiment_data = read_file(input_file)

  tokenizer, X_train, X_test, y1_train, y1_test, y2_train, y2_test, y3_train, y3_test, y4_train, y4_test = create_dataset(sentiment_data)
  
  try:
    # Load the model
    #filepath = '/Users/riyasimon/Documents/UIUC/CS410/project/code/sentiment-analysis/saved_model'
    model = load_model(model_file, compile = False)
  except:
    embedding_matrix = embeddings(tokenizer)
    model = create_model(tokenizer, embedding_matrix)
    run_model(model, X_train, y1_train, y2_train, y3_train, y4_train)
    evaluate_model(model, X_test, y1_test, y2_test, y3_test, y4_test)
    save_sentiment_model(model)

  return predict(model, tokenizer, [sample_data])

if __name__ == "__main__":
  import sys

  '''
  input_file = sys.argv[1]
  sample_args = sys.argv[2:]
  sample_data = ''
  for i in sample_args:
      sample_data += i + ' '
  sample_data = sample_data[0:len(sample_data)-1]
  '''
  input_file = 'reviewsABSA.csv'
  sample_data1 = "Poorly structured and clearly not thought through or designed for the online format "
  sample_data2 = "Angrave is a great professor. He definitely does his best to keep the class interesting, and he does a great job of it. The MPs are helpful and the tests are fair. Overall, its a very easy class if you have prior programming experience, but definitely doable even if you don't. Very nice, funny, and intelligent professor."
  sample_data3 = "I have learned some good points about financial markets, some aspects of how corporation works and how they are governed, as well as different types of corporations were very interesting. i was also surprised how much i knew already, without having any kind of finance background. I guess it's four stars cause i was expecting more maths and statistics and there is very little of that( just in the beginning) it is still very interesting though. As a science graduate i was surprised that same formulas used in statistics in biology, can be applied to the financial markets. The course if worth a try especially  for beginners in financial markets but also I would make essential for general public, to broaden their knowledge about finances. I also wish there was one or two more peer reviewed assignments. I think it was great to read other people work as views have differed so much! Also the peer review should have few more options for scoring points. but other than that it was a really interesting. Very good opportunity to get into civilised debate. "
  #main(input_file, [sample_data1], "./saved_model")
  #main(input_file, [sample_data1], "saved_model")
  main(input_file, [sample_data2], "saved_model")
  #main(input_file, [sample_data3], "saved_model")

