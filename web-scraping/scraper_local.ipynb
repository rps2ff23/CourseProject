{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXfRTPvm1PYP"
      },
      "source": [
        "# Scraping Rate My Professor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST3mKndJ1PYR"
      },
      "source": [
        "## System setup \n",
        "\n",
        "Before we start, make sure to install the required libraries\n",
        "    \n",
        "    pip install bs4\n",
        "    pip install selenium\n",
        "\n",
        "For Chrome, I also downloaded the appropriate webdriver from here: http://chromedriver.chromium.org/downloads, unzip it and save in current directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iutgvNQK1lue",
        "outputId": "de5a6d66-664a-4221-b29e-49da1082008a"
      },
      "outputs": [],
      "source": [
        "#!pip install bs4\n",
        "#!pip install selenium\n",
        "#!apt install chromium-chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hwh19yP1PYS"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver \n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from RateProfHelper import RateMyProfScraper\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "from collections import OrderedDict\n",
        "#from selenium.webdriver.chrome.service import Service\n",
        "\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "#from selenium.webdriver.chrome.service import Service\n",
        "import re \n",
        "import urllib\n",
        "import time\n",
        "import unicodecsv as csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrAxJSme3l2Q",
        "outputId": "fe80896b-081a-4fb0-8b3f-e7672d0eaf4b"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPU6NpeV69kf"
      },
      "outputs": [],
      "source": [
        "#!chmod 755 /content/drive/MyDrive/CS410Project/colab/chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZLMtgOD1PYT"
      },
      "outputs": [],
      "source": [
        "#create a webdriver object and set options for headless browsing\n",
        "options = Options()\n",
        "options.headless = True\n",
        "#options.add_argument('--no-sandbox')\n",
        "#options.add_argument('--disable-dev-shm-usage')\n",
        "#service = Service(\"chromedriver\")\n",
        "driver = webdriver.Chrome(\"./chromedriver\",options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAtJKMMe1PYT"
      },
      "source": [
        "Before we start scraping, we'll define some helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJsRnuOf1PYU"
      },
      "outputs": [],
      "source": [
        "#uses webdriver object to execute javascript code and get dynamically loaded webcontent\n",
        "def get_js_soup(url,driver):\n",
        "    driver.get(url)\n",
        "    res_html = driver.execute_script('return document.body.innerHTML')\n",
        "    soup = BeautifulSoup(res_html,'html.parser') #beautiful soup object to be used for parsing html content\n",
        "    return soup\n",
        "\n",
        "#tidies extracted text \n",
        "def process_bio(bio):\n",
        "    bio = bio.encode('ascii',errors='ignore').decode('utf-8')       #removes non-ascii characters\n",
        "    bio = re.sub('\\s+',' ',bio)       #repalces repeated whitespace characters with single space\n",
        "    return bio\n",
        "\n",
        "''' More tidying\n",
        "Sometimes the text extracted HTML webpage may contain javascript code and some style elements. \n",
        "This function removes script and style tags from HTML so that extracted text does not contain them.\n",
        "'''\n",
        "def remove_script(soup):\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.decompose()\n",
        "    return soup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qy-XB7Q1PYV"
      },
      "source": [
        "We will now start scraping.\n",
        "\n",
        "Using the helper class RateMyProfScraper, we can get a list of all the professors at UIUC who have more than 30 reviews. We can use the tid, metadata information from the request to create the corresponding urls which we will use later to scrape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UIUC = RateMyProfScraper(1112)\n",
        "#for professor in UIUC.professorlist:\n",
        "#    print(professor)\n",
        "#UIUC.SearchProfessor(\"Laura Hill\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(UIUC.professorlist))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fetching url with \"tid\" for each professor, needed for finding the rate my professor url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic-q0fMv1PYV"
      },
      "outputs": [],
      "source": [
        "#extracts all Faculty Profile page urls from the Directory Listing Page\n",
        "def scrape_dir_page(driver, professor):\n",
        "    base_url = 'https://www.ratemyprofessors.com/ShowRatings.jsp?tid='\n",
        "    curr_page = base_url + str(professor['tid'])\n",
        "    professor_name = professor['tFname'] + \" \" + professor['tLname']\n",
        "    faculty_links.append({ professor_name: curr_page }) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a list of dictionaries with keys [{\"professor name\":\"test\", \"faculty url\":\"testurl\"}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KsRuSSf1PYX",
        "outputId": "d3a4811a-0575-4638-bab8-6e17c009c976"
      },
      "outputs": [],
      "source": [
        "#dir_url = 'https://engineering.virginia.edu/departments/electrical-and-computer-engineering/electrical-computer-engineering-faculty?page=' #url of directory listings of CS faculty\n",
        "faculty_links = []\n",
        "for professor in UIUC.professorlist:\n",
        "    scrape_dir_page(driver, professor)\n",
        "\n",
        "print ('-'*20,'Scraping directory page','-'*20)\n",
        "print ('-'*20,'Found {} faculty profile urls'.format(len(faculty_links)),'-'*20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(faculty_links[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function that loads each faculty url, pushes \"load more comments\" button multiple times, and gets the final result list of comments\n",
        "\n",
        "Fetches\n",
        "- comment\n",
        "- rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJDsL1_b1PYY"
      },
      "outputs": [],
      "source": [
        "def scrape_faculty_page(professor_name, fac_url,driver):\n",
        "    driver.get(fac_url)\n",
        "    homepage_found = False\n",
        "    reviews = []\n",
        "    bio = ''\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            element = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//button[normalize-space()=\"Load More Ratings\"]')))\n",
        "            loadmore = driver.find_element_by_xpath('//button[normalize-space()=\"Load More Ratings\"]')\n",
        "            actions = ActionChains(driver)\n",
        "            actions.move_to_element(loadmore).perform()\n",
        "            actions.click().perform()\n",
        "            #driver.execute_script(\"arguments[0].scrollIntoView();\", loadmore)\n",
        "            #driver.execute_script(\"arguments[0].click();\", loadmore)\n",
        "            #print(\"clicking\")\n",
        "        except TimeoutException:\n",
        "            print(\"End of loading\")\n",
        "            break\n",
        "        except NoSuchElementException:\n",
        "            print(\"End of loading\")\n",
        "            break\n",
        "\n",
        "    res_html = driver.execute_script('return document.body.innerHTML')\n",
        "    soup = BeautifulSoup(res_html,'html.parser') #beautiful soup object to be used for parsing html content\n",
        "    results = soup.find(id=\"ratingsList\")\n",
        "\n",
        "    for li in results:\n",
        "        comment = li.find(\"div\", class_=\"Comments__StyledComments-dzzyvm-0 gRjWel\")\n",
        "        rating = li.find(\"div\", class_=[\"CardNumRating__CardNumRatingNumber-sc-17t4b9u-2 kMhQxZ\",\"CardNumRating__CardNumRatingNumber-sc-17t4b9u-2 bUneqk\"])\n",
        "        tags = li.find(\"div\", class_=\"RatingTags__StyledTags-sc-1boeqx2-0 eLpnFv\")\n",
        "\n",
        "        if comment and rating and tags:\n",
        "            review = OrderedDict()\n",
        "            review['professor'] = professor_name\n",
        "            review['comment'] = comment.text.strip()\n",
        "            review['rating'] = rating.text.strip()\n",
        "            tags_list = []\n",
        "            for span in tags.select('span'):\n",
        "                tags_list.append(span.text.strip())\n",
        "            review['tags'] = '|'.join(tags_list)\n",
        "            reviews.append(review)\n",
        "    \n",
        "    return reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finding the comments and ratings for each professor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#reviews = scrape_faculty_page(\"Gretchen Adams\", \"https://www.ratemyprofessors.com/ShowRatings.jsp?tid=477677\", driver)\n",
        "#print(reviews)\n",
        "all_reviews = []\n",
        "for link in faculty_links[0:150]:\n",
        "    for item in link.items():\n",
        "        print ('-'*20,'Scraping faculty url {}'.format(item[1]),'-'*20)\n",
        "        reviews = scrape_faculty_page(item[0], item[1], driver)\n",
        "        all_reviews.extend(reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Writing the results to CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(all_reviews[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('spreadsheet_150_tags.csv', 'w') as outfile:\n",
        "    fp = csv.DictWriter(outfile, all_reviews[0].keys(), encoding='utf-8')\n",
        "    fp.writeheader()\n",
        "    fp.writerows(all_reviews)\n",
        "    #fp.writerow([unicode(s, \"utf-8\") for s in all_reviews])\n",
        "    #fp.writerow([str(text, 'utf-8') for s in all_reviews])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "scraper.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    },
    "kernelspec": {
      "display_name": "Python 2.7.16 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
